{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WCMXPdhifmGR"
   },
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6AQKyacCz0SL"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import scipy.constants as constant\n",
    "\n",
    "# Carrier Frequency \n",
    "FC = 2e9  \n",
    "\n",
    "# Speed of Light\n",
    "C = constant.c  \n",
    "\n",
    "# Power Spectral Density\n",
    "N0 = 1e-20  \n",
    "\n",
    "# Bandwidth\n",
    "B = 1e6  \n",
    "\n",
    "# Noise\n",
    "STD = B * N0  \n",
    "\n",
    "# Environmental Variable 1\n",
    "B1 = 0.36  \n",
    "\n",
    "# Environmental Variable 2\n",
    "B2 = 0.21  \n",
    "\n",
    "# Path Loss Exponent\n",
    "ALPHA = 2   \n",
    "\n",
    "# Additional path loss for LineOfSight\n",
    "U_LOS = 10**(3/10)  \n",
    "\n",
    "# Additional path loss for NonLineOfSight\n",
    "U_NLOS = 10**(23/10)  \n",
    "\n",
    "K0 = (4 * math.pi * FC / C) ** 2\n",
    "\n",
    "# Environmental Variable 3\n",
    "ZETA = 0  \n",
    "\n",
    "# Transmission power of an UAV\n",
    "P = 0.08  \n",
    "\n",
    "# Discount Rate\n",
    "GAMMA = 0.95  \n",
    "\n",
    "# Learning Rate\n",
    "LR = 0.001  \n",
    "\n",
    "# Initial Epsilon Greedy Value\n",
    "EPSILON = 1  \n",
    "\n",
    "# Epsilon Greedy Decay Rate\n",
    "EPSILON_DECAY = 0.95  \n",
    "\n",
    "# Minimum Epsilon Greedy\n",
    "EPSILON_MIN = 0.1  \n",
    "\n",
    "# Minimum altitude to fly\n",
    "MIN_ALTITUDE = 200\n",
    "\n",
    "# Initial Altitude\n",
    "ALTITUDE = 400  \n",
    "\n",
    "# Maximum altitude to fly\n",
    "MAX_ALTITUDE = 800  \n",
    "\n",
    "# Penalty value\n",
    "PENALTY = -100   \n",
    "\n",
    "# UAV speed rate according to humans\n",
    "UAV_MOVEMENT_SCALE= 20\n",
    "\n",
    "TEST_ENV_FILE = \"User_Data/test_env.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bgRhcYK3fG_w"
   },
   "outputs": [],
   "source": [
    "import random as rand \n",
    "import tensorflow as tf\n",
    "from time import sleep\n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from collections import deque \n",
    "import time\n",
    "class Agent(): \n",
    "    def __init__(self,state_size,action_count,batch_size=200,maxlen=10000):\n",
    "        self.batch_size = batch_size\n",
    "        self.input_size   = state_size \n",
    "        self.action_count = action_count \n",
    "        self.lr = LR\n",
    "        self.gamma = GAMMA\n",
    "        self.epsilon = EPSILON\n",
    "        self.epsilon_decay = EPSILON_DECAY\n",
    "        self.epsilon_min = EPSILON_MIN\n",
    "        self.model = self.build_model() # Deep Q Network\n",
    "        self.memory = deque(maxlen=maxlen) # Experience Memory\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"\n",
    "            Create Neural network as Deep Q Network\n",
    "        \"\"\"    \n",
    "        model = Sequential()\n",
    "        model.add(Dense(128, input_dim= self.input_size, activation = \"relu\"))   \n",
    "        model.add(Dense(64,activation=\"relu\"))   \n",
    "        model.add(Dense(64,activation=\"relu\"))   \n",
    "        model.add(Dense(64,activation=\"relu\"))  \n",
    "        model.add(Dense(64,activation=\"relu\"))  \n",
    "        model.add(Dense(self.action_count,activation=\"linear\"))\n",
    "        model.compile(Adam(learning_rate = self.lr ,clipnorm=1.0),loss=Huber())\n",
    "        return model  \n",
    "\n",
    "    def store(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "            Store experience in memory\n",
    "        \"\"\"\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self,state,cannot_random=False):\n",
    "        \"\"\"\n",
    "            Choosing action either random or using Deep Q Network\n",
    "        \"\"\"\n",
    "        if (rand.uniform(0,1) <= self.epsilon and not cannot_random):\n",
    "            return rand.randint(0,self.action_count-1) \n",
    "        else:\n",
    "            act_values = self.model.predict(state) \n",
    "            return np.argmax(act_values[0])\n",
    "        \n",
    "    def is_memory_enough(self):\n",
    "        \"\"\"\n",
    "            Check if memory is full enough for a batch\n",
    "        \"\"\"\n",
    "        return not (len(self.memory)< self.batch_size)\n",
    "    \n",
    "    def replay(self):\n",
    "        \"\"\"\n",
    "            Sample random experiences from memory and replay them\n",
    "        \"\"\"\n",
    "        if(not self.is_memory_enough()):\n",
    "            return\n",
    "        \n",
    "        minibatch = rand.sample(self.memory,self.batch_size)\n",
    "        \n",
    "        for state,action, reward,next_state, done in minibatch:  \n",
    "            \n",
    "          if done:\n",
    "            target = reward\n",
    "          else:  \n",
    "            target = reward + self.gamma*np.amax(self.model.predict(next_state))  \n",
    "           \n",
    "          train_target = self.model.predict(state)\n",
    "          train_target[0][action] = target \n",
    "          \n",
    "          self.model.fit(state,train_target,verbose=0,workers=8,use_multiprocessing=True)\n",
    "\n",
    "    def adaptiveEGreedy(self):\n",
    "        \"\"\"\n",
    "            Decay epsilon\n",
    "        \"\"\"\n",
    "        if(self.epsilon > self.epsilon_min and self.is_memory_enough()):\n",
    "            self.epsilon *= self.epsilon_decay   \n",
    "        print(\"current_randomness: \", self.epsilon)\n",
    "\n",
    "from scipy.stats import truncnorm\n",
    "\n",
    "# Helper Function for normal distribution\n",
    "def get_truncated_normal(mean=0, low=0, upp=10):\n",
    "    return truncnorm(\n",
    "        (low - mean), (upp - mean), loc=mean, scale=1).rvs()\n",
    "    \n",
    "# Normal Distribution\n",
    "def gaussian(end,begin=0):\n",
    "    normal_random = round(get_truncated_normal(mean=int((begin+end)/2),low=begin,upp=end))\n",
    "    return max(0,min(normal_random,end))\n",
    "\n",
    "\n",
    "# General Class for moving objects to extend from.\n",
    "class MovingObject():\n",
    "    def __init__(self,initial_location):\n",
    "        self.current_location = initial_location.copy()\n",
    "        self.starting_location = initial_location.copy()\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_location = self.starting_location.copy()\n",
    "\n",
    "\n",
    "#UAV class\n",
    "class UAV(Agent,MovingObject): \n",
    "    def __init__(self,state_size,action_count,batch_size,initial_location,maxlen=10000):\n",
    "        Agent.__init__(self,state_size,action_count,batch_size=batch_size,maxlen=maxlen)\n",
    "        MovingObject.__init__(self,initial_location)\n",
    "        self.transmission_power = P\n",
    "\n",
    "\n",
    "#User class\n",
    "class UE(MovingObject):\n",
    "  def __init__(self,initial_location,env_dim,movement_vectors,define_path_first = True,step_count = 10000,movement_function=gaussian):\n",
    "    super().__init__(initial_location)\n",
    "\n",
    "    self.movement_vectors =movement_vectors[:5].copy()\n",
    "    rand.shuffle(self.movement_vectors)\n",
    "\n",
    "    self.movement_function = movement_function\n",
    "    self.maxY , self.maxX = env_dim\n",
    "    self.path = []\n",
    "    self.path_determined = define_path_first\n",
    "\n",
    "    if define_path_first:\n",
    "      self.initial = 0\n",
    "      for _ in range(step_count):\n",
    "        self.path.append(movement_function(len(self.movement_vectors)-1))\n",
    "    \n",
    "  def move(self):\n",
    "    action_index = 0\n",
    "    if self.path_determined:\n",
    "      action_index = self.path[self.initial]\n",
    "      self.initial+=1\n",
    "      if self.initial==len(self.path):\n",
    "        self.initial = 0\n",
    "    else:\n",
    "      action_index = self.movement_function(len(self.movement_vectors)-1)\n",
    "\n",
    "    action = self.movement_vectors[int(action_index)]\n",
    "    self.current_location= [self.current_location[i] + action[i] for i in range(3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RHsK4i8vN-wc"
   },
   "source": [
    "# Environment Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MVYDnGuofLUA"
   },
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import math\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "import time\n",
    "from keras.models import load_model\n",
    "import pickle\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "\n",
    "class UavSimuEnv():\n",
    "  actions = [(1,0,0),(-1,0,0),(0,0,0),(0,1,0),(0,-1,0)]  # Actions\n",
    "  coord_count = 2\n",
    "      \n",
    "  def distance_func(self,v1,v2): \n",
    "    \"\"\" x and y is unit, altitude is meter so calculated distance this way \"\"\"\n",
    "    return (sum(((p-q)*UNIT)**2 for p, q in zip(v1[:2], v2[:2])) + (v1[2]-v2[2])**2)** .5\n",
    "\n",
    "  def get_all_uavs(self):\n",
    "        return [uav for uav in self.map[\"uav_set\"]]\n",
    "        \n",
    "  def save(self,env_name): \n",
    "    \"\"\"\n",
    "      save models and memories of all UAVs\n",
    "    \"\"\"\n",
    "    for index,uav in enumerate(self.get_all_uavs()):\n",
    "        model = uav.model\n",
    "        path = \"modelss/{}/\".format(env_name)\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        model_name = \"{}uav{}\".format(path,index)\n",
    "        model.save(model_name+\".h5\")\n",
    "        pickle.dump(uav.memory, open(model_name+\"memory.pkl\", 'wb'))\n",
    "            \n",
    "            \n",
    "  def load(self,env_name,just_for_test=True):\n",
    "    \"\"\"\n",
    "      load models and memories for all UAVs\n",
    "    \"\"\"\n",
    "    for index in range(len(self.map[\"uav_set\"])):\n",
    "        path = \"modelss/{}/\".format(env_name)\n",
    "        model_name = \"{}uav{}\".format(path,index)\n",
    "        self.getUAV(index).model = keras.models.load_model(model_name+\".h5\",compile=just_for_test)\n",
    "        self.getUAV(index).memory = pickle.load(open(model_name+\"memory.pkl\", \"rb\"))\n",
    "\n",
    "            \n",
    "  def get_input_size(self,uav_count):\n",
    "    return uav_count*self.coord_count + math.ceil(self.step_count/UAV_MOVEMENT_SCALE)\n",
    "\n",
    "  def is_collect_step(self):\n",
    "    return self.getUAV(0).is_memory_enough() \n",
    "               \n",
    "  def get_current_epsilon(self):\n",
    "    return self.getUAV(0).epsilon\n",
    "    \n",
    "  def __init__(self,uav_paths,ue_set,env_dim = (100,100),batch_size=200,max_memory_per_agent=10000,uav_class=UAV):\n",
    "    self.step_count = STEP_COUNT\n",
    "    self.uav_count = len(uav_paths) \n",
    "    self.env_dim = env_dim \n",
    "    self.input_size = self.get_input_size(len(uav_paths))\n",
    "    self.map = {\"uav_set\": self.init_uav(uav_paths,batch_size,max_memory_per_agent,uav_class), \"ue_set\": ue_set}\n",
    "\n",
    "  def init_uav(self,uav_paths,batch_size,max_memory_per_agent,uav_class):\n",
    "    \"\"\"\n",
    "      Initialize Agents\n",
    "    \"\"\"\n",
    "    print(uav_class)\n",
    "    return [uav_class(self.input_size, len(self.actions), batch_size, begin,maxlen=max_memory_per_agent) \n",
    "            for begin in uav_paths]\n",
    "\n",
    "\n",
    "  def get_distance_list(self, dest_list, location,isObject=False):\n",
    "    \"\"\"\n",
    "      Return distance list of distance between the locations in dest;_list and given location\n",
    "    \"\"\"\n",
    "    if (isObject):\n",
    "      return [self.distance_func(location,loc.current_location) for loc in dest_list]\n",
    "    return [self.distance_func(location,loc) for loc in dest_list] \n",
    "    \n",
    "  def reset(self):\n",
    "    \"\"\"\n",
    "      Reset Environment\n",
    "    \"\"\"\n",
    "    def reset_set(key):\n",
    "      for index, val in enumerate(self.map[key]):\n",
    "        self.map[key][index].reset()\n",
    "    \n",
    "    reset_set(\"ue_set\")\n",
    "    reset_set(\"uav_set\")\n",
    "    self.initial_time = 0 \n",
    "\n",
    "  def get_state_input(self,isNext=False):\n",
    "    \"\"\"\n",
    "      Return state information for UAV\n",
    "    \"\"\"\n",
    "    all_uav_coordinates = []\n",
    "\n",
    "    for uav in self.map[\"uav_set\"]:\n",
    "      all_uav_coordinates.extend(uav.current_location[:self.coord_count]) \n",
    "    \n",
    "    time_range = [0 for _ in range(math.ceil(self.step_count/UAV_MOVEMENT_SCALE))]\n",
    "    print(self.initial_time // UAV_MOVEMENT_SCALE)\n",
    "    print(self.initial_time)\n",
    "    time_range[self.initial_time // UAV_MOVEMENT_SCALE] = 1\n",
    "    return self.reshape(all_uav_coordinates +time_range)\n",
    "\n",
    "    \n",
    "  def get_distance_matrix(self,map_obj=None):\n",
    "    \"\"\"\n",
    "      Return all distance matrix and association count vector between UAV-UE \n",
    "    \"\"\"\n",
    "    if(map_obj==None):\n",
    "        map_obj = self.map\n",
    "        \n",
    "    def minIndex(lst):\n",
    "      return lst.index(min(lst))    \n",
    "\n",
    "    def get_distances(lst1, lst2, isLst2Object = True):\n",
    "      distance_matrix = []\n",
    "      assoc_matrix = []\n",
    "      for _ in range(len(lst2)):\n",
    "        assoc_matrix.append([])\n",
    "\n",
    "      for index,member in enumerate(lst1):\n",
    "        distances = self.get_distance_list(lst2,member.current_location,isLst2Object)\n",
    "        distance_matrix.append(distances)\n",
    "        index_of_min = minIndex(distances)\n",
    "        assoc_matrix[index_of_min].append(index)\n",
    "\n",
    "      return distance_matrix, assoc_matrix\n",
    "\n",
    "    ue_uav_matrix, assoc_matrix_uav = get_distances(map_obj[\"ue_set\"],map_obj[\"uav_set\"]) \n",
    "\n",
    "    return {\"ue_uav_matrix\": ue_uav_matrix, \"assoc_matrix_uav\" : assoc_matrix_uav}\n",
    "\n",
    "\n",
    "  def calculate_sum_rate(self,map_obj=None):\n",
    "    \n",
    "    if(map_obj==None):\n",
    "        map_obj = self.map\n",
    "        \n",
    "    uav_count = len(map_obj[\"uav_set\"])\n",
    "    ue_count = len(map_obj[\"ue_set\"])\n",
    "    distance_state_dict = self.get_distance_matrix(map_obj)\n",
    "    sumrate = 0\n",
    "    transmit_powers = []\n",
    "    channel_gain_matrix = []\n",
    "    \n",
    "    # Nested Function\n",
    "    def calculate_transmit_power(uav_index):\n",
    "      \"\"\"\n",
    "        calculate transmit power of uav\n",
    "      \"\"\"\n",
    "      uav = map_obj[\"uav_set\"][uav_index]\n",
    "      uav_assoc_count = len(distance_state_dict[\"assoc_matrix_uav\"][uav_index])\n",
    "    \n",
    "      #Eğer uav hiçbir servis yapmıyorsa transmit gücünü kullanmayacağı için 0 olacak\n",
    "      if(uav_assoc_count==0):\n",
    "            return 0 \n",
    "      p = uav.transmission_power / uav_assoc_count\n",
    "\n",
    "      return p\n",
    "\n",
    "    # Nested Function\n",
    "    def calculate_channel_gain(uav_index,user_index):\n",
    "      \"\"\"\n",
    "        calculate channel gain between uav and user\n",
    "      \"\"\"\n",
    "      uav = map_obj[\"uav_set\"][uav_index]\n",
    "      d = distance_state_dict[\"ue_uav_matrix\"][user_index][uav_index]\n",
    "      theta = math.asin(uav.current_location[-1] / d )\n",
    "      Plos = B1 * (180 * theta / math.pi - ZETA ) ** B2\n",
    "      \n",
    "      Pnlos = 1 - Plos\n",
    "      g = (K0 ** (-1)) * (d ** (- ALPHA)) * ((Plos*U_LOS + Pnlos*U_NLOS) **(-1))\n",
    "      return g\n",
    "\n",
    "\n",
    "    #First calculate all channel gains and transmit powers of all combination of uav and users\n",
    "    for uav_index in range(uav_count):\n",
    "\n",
    "      p = calculate_transmit_power(uav_index)\n",
    "      transmit_powers.append(p)\n",
    "      channel_gain_list = [calculate_channel_gain(uav_index,user_index) \\\n",
    "                           for user_index in range(ue_count)]\n",
    "      channel_gain_matrix.append(channel_gain_list)\n",
    "    \n",
    "    # Nested Function\n",
    "    def calculate_interference(uav_index,user_index):\n",
    "      \"\"\"\n",
    "        Calculate interference between uav and user caused by other uavs\n",
    "      \"\"\"\n",
    "      I = 0\n",
    "\n",
    "      for other_uav_index in range(uav_count):\n",
    "        if (other_uav_index==uav_index):\n",
    "          continue\n",
    "        p = transmit_powers[other_uav_index]\n",
    "        g = channel_gain_matrix[other_uav_index][user_index]\n",
    "        I +=  p*g \n",
    "        \n",
    "      return I\n",
    "    \n",
    "\n",
    "    for uav_index in range(uav_count):\n",
    "\n",
    "        p = transmit_powers[uav_index]\n",
    "        users_of_uav = distance_state_dict[\"assoc_matrix_uav\"][uav_index]\n",
    "        for user_index in users_of_uav:\n",
    "          \n",
    "          I = calculate_interference(uav_index,user_index)\n",
    "          g = channel_gain_matrix[uav_index][user_index]\n",
    "          SINR = p*g/(I + STD)\n",
    "          sumrateOfUser = B * math.log2(1+SINR)\n",
    "          sumrate += sumrateOfUser \n",
    "            \n",
    "    return sumrate*1e-7\n",
    "\n",
    "  def step_UEs(self):\n",
    "    \"\"\"\n",
    "      Move all UEs in Simulation\n",
    "    \"\"\"\n",
    "    ue_set_length = len(self.map[\"ue_set\"])\n",
    "    for ue in self.map[\"ue_set\"]:\n",
    "      ue.move()\n",
    "\n",
    "      if (not self.isInside(ue.current_location,True)):\n",
    "        self.map[\"ue_set\"].remove(ue)\n",
    "\n",
    "\n",
    "  def isCollides(self,uav_index,new_location): \n",
    "    \"\"\"\n",
    "      check if location is against limits\n",
    "    \"\"\"\n",
    "    for index,uav in enumerate(self.map[\"uav_set\"]): \n",
    "        if(index!=uav_index and np.array_equal(uav.current_location, new_location)):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "  def step_env(self,action_indexes):\n",
    "        \"\"\"\n",
    "          Change environment with given joint actions\n",
    "        \"\"\"\n",
    "        old_sum_rate = self.calculate_sum_rate()\n",
    "        penalty = 0\n",
    "        done = False\n",
    "        for uav_index in range(len(self.map[\"uav_set\"])):\n",
    "            current = self.getUAV(uav_index).current_location\n",
    "            action_index = action_indexes[uav_index]\n",
    "            new_location = [current[i] + self.actions[action_index][i]\n",
    "                                                         for i in range(3)]\n",
    "            if(not self.isInside(new_location)):\n",
    "                penalty += PENALTY\n",
    "                done = True\n",
    "            else:\n",
    "                self.getUAV(uav_index).current_location = new_location\n",
    "        \n",
    "        for uav_index in range(len(self.map[\"uav_set\"])):\n",
    "            if(self.isCollides(uav_index,self.getUAV(uav_index).current_location)):\n",
    "                done = True\n",
    "                penalty +=PENALTY\n",
    "                \n",
    "        new_sum_rate = self.calculate_sum_rate()\n",
    "        \n",
    "        reward = new_sum_rate-old_sum_rate + penalty #penalty negatif\n",
    "        done = done or self.initial_time == self.step_count\n",
    "        return self.get_state_input(), reward, done\n",
    "    \n",
    "     \n",
    "  def getUAV(self,uav_index):\n",
    "    return self.map[\"uav_set\"][uav_index]\n",
    "\n",
    "\n",
    "  def step_UAVs(self,isTest=False,isCollectStep=False):\n",
    "    \"\"\"\n",
    "      Move all UAVs\n",
    "    \"\"\"\n",
    "    uav_set_length = len(self.map[\"uav_set\"])\n",
    "    \n",
    "    state = self.get_state_input()\n",
    "  \n",
    "    action_indexes = [self.getUAV(uav_index).act(state,cannot_random = isTest) \\\n",
    "               for uav_index in range(uav_set_length)]\n",
    "        \n",
    "    next_state, reward, Done = self.step_env(action_indexes)\n",
    "  \n",
    "    if(not isTest):\n",
    "        for uav_index in range(uav_set_length):\n",
    "            self.getUAV(uav_index).store(state,action_indexes[uav_index],reward,next_state,Done) \n",
    "            \n",
    "            if(not isCollectStep):\n",
    "                self.getUAV(uav_index).replay()\n",
    "                if(self.initial_time==self.step_count-1):\n",
    "                    self.getUAV(uav_index).adaptiveEGreedy() \n",
    "    self.initial_time +=1\n",
    "        \n",
    "            \n",
    "            \n",
    "  def reshape(self, data):\n",
    "    return np.reshape(data, [1,self.input_size]) \n",
    "\n",
    "\n",
    "  def isInside(self,location,isOnGround=False):\n",
    "    \"\"\"\n",
    "      Return if location is against the limits\n",
    "    \"\"\"\n",
    "    return (0 <= location[0] < self.env_dim[1] and 0<= location[1] < self.env_dim[0] ) \\\n",
    "            and (isOnGround or  MIN_ALTITUDE <= location[2] <= MAX_ALTITUDE)\n",
    "\n",
    "  def get3DMap(self):\n",
    "      def getCoords(map):\n",
    "        x,y,z = [],[],[]\n",
    "        for obj in map:\n",
    "          x.append(obj.current_location[0])\n",
    "          y.append(obj.current_location[1])\n",
    "          z.append(obj.current_location[2])\n",
    "        return x,y,z\n",
    "      return getCoords(self.map[\"uav_set\"]),getCoords(self.map[\"ue_set\"])\n",
    "\n",
    "    \n",
    "  def render(self):\n",
    "\n",
    "    \"\"\"\n",
    "      Render simulation visually\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = Axes3D(fig)\n",
    "    uav_coords,ue_coords = self.get3DMap()\n",
    "    clear_output(wait=True)\n",
    "    ax.set_xlim3d(0,self.env_dim[0])\n",
    "    ax.set_ylim3d(0,self.env_dim[1])\n",
    "    ax.set_zlim3d(0,MAX_ALTITUDE)\n",
    "    ax.scatter(*uav_coords,c=\"green\")\n",
    "    ax.scatter(*ue_coords,c=\"red\")\n",
    "    distance_map = self.get_distance_matrix()[\"assoc_matrix_uav\"]\n",
    "    for uav_index in range(self.uav_count):\n",
    "        coord_of_UAV = self.getUAV(uav_index).current_location\n",
    "        for ue_index in distance_map[uav_index]: \n",
    "            coord_of_UE = self.map[\"ue_set\"][ue_index].current_location\n",
    "            ax.plot([coord_of_UE[0],coord_of_UAV[0]],[coord_of_UE[1],coord_of_UAV[1]],[coord_of_UE[2],coord_of_UAV[2]],c=\"green\",alpha=.1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "umC3ZrHAfSH9"
   },
   "source": [
    "# Simulation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H0v-PiXCKj8h"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "from keras.optimizers import Adam \n",
    "import multiprocessing\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def plot_iter_graph(ignore_count,sum_rates_per_iteration):\n",
    "    plt.plot(sum_rates_per_iteration[ignore_count:])\n",
    "    plt.title(\"Sum rates per iteration\")\n",
    "    plt.show(sum_rates_per_iteration)\n",
    "\n",
    "def plot_step_graph(iteration_num, step_num,step_count,sum_rates_per_step):\n",
    "  if (iteration_num >= 1): \n",
    "    if (step_num >1):\n",
    "      begin_iter = max(0,iteration_num-5)\n",
    "      x_pivot = begin_iter*step_count\n",
    "      print(begin_iter,x_pivot,iteration_num)\n",
    "      show_range = range(x_pivot , len(sum_rates_per_step))\n",
    "      plt.plot(show_range,sum_rates_per_step[x_pivot:])\n",
    "      plt.title(\"Sum rates per step\") \n",
    "      for t in range(5):\n",
    "        plt.axvline(x=(t+begin_iter)*step_count,color=\"red\")\n",
    "    \n",
    "      plt.show()\n",
    "\n",
    "# Train environment\n",
    "def simulate(env,iteration_count = 50, step_count = 20,batch_size=200,consider_terminal_state=False,collect_step_size=100,\n",
    "            env_name=None,onlyForTest=False):\n",
    "\n",
    "  sum_rates_per_iteration = []\n",
    "  sum_rates_per_step = []\n",
    "  max_score = 0\n",
    "  max_other_score = 0\n",
    "  for iter_num in range(iteration_count + (collect_step_size//step_count)):\n",
    "    \n",
    "    \n",
    "    counter = 0  \n",
    "    isCollectStep = collect_step_size > (iter_num*step_count)\n",
    "    env.test_mode = False\n",
    "    env.reset() \n",
    "    while (not onlyForTest and counter!=step_count and (isCollectStep == (collect_step_size > (iter_num*step_count + counter) ))):\n",
    "        \n",
    "        \n",
    "        \"\"\"if counter%UAV_MOVEMENT_SCALE==0:\n",
    "          env.step_UEs()\"\"\"\n",
    "        \n",
    "        #Step simulation\n",
    "        env.step_UAVs(isCollectStep=isCollectStep) \n",
    "        counter += 1\n",
    "        \n",
    "        #Plot sum rates by step and interation\n",
    "        clear_output(wait=False)\n",
    " \n",
    "        plot_iter_graph(0,sum_rates_per_iteration)\n",
    "        plot_step_graph(iter_num,counter,step_count,sum_rates_per_step)\n",
    "        \n",
    "        print(env.get_current_epsilon())\n",
    "        print(\"Training, step: \",counter)\n",
    "        print(\"Iteration: \",iter_num)\n",
    "         \n",
    "\n",
    "    #TESTING\n",
    "    counter = 0\n",
    "    total_sum_rate = 0\n",
    "    env.test_mode = True\n",
    "    env.reset()\n",
    "    while (counter!=step_count and not isCollectStep):\n",
    "\n",
    "        \"\"\"if counter%UAV_MOVEMENT_SCALE==0:\n",
    "          env.step_UEs()\"\"\"\n",
    "        #Step UAVs\n",
    "        env.step_UAVs(isTest=True)\n",
    "\n",
    "        #Get Error\n",
    "        sum_rate = env.calculate_sum_rate()\n",
    "        sum_rates_per_step.append(sum_rate)\n",
    "        total_sum_rate += sum_rate\n",
    "        counter+=1\n",
    "        #Step UEs \n",
    "        env.render() \n",
    "        \n",
    "\n",
    "        #Plot resource errors by step and interation\n",
    "        plot_step_graph(iter_num,counter,step_count,sum_rates_per_step)\n",
    "        \n",
    "        print(\"Testing...\")\n",
    "        print(\"Counter: \",counter)\n",
    "        print(\"Iteration Number: \",iter_num)\n",
    "        \n",
    "    print(total_sum_rate/step_count)\n",
    "    if(not isCollectStep):\n",
    "        score = total_sum_rate/step_count\n",
    "        other_score = sum_rates_per_step[-1]\n",
    "        sum_rates_per_iteration.append(score)\n",
    "        if(score > max_score):\n",
    "            env.save(env_name)\n",
    "            max_score = score \n",
    "        if(other_score > max_other_score):\n",
    "            env.save(env_name+\"_bestend\")\n",
    "            max_other_score = other_score \n",
    "  return sum_rates_per_iteration, sum_rates_per_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_gFJRd3Ddho"
   },
   "source": [
    "# Extended Environment and Agent Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S4JEO27Fa8AQ"
   },
   "outputs": [],
   "source": [
    "class ThreeDOFUavSimu(UavSimuEnv):\n",
    "    coord_count = 3\n",
    "    actions = [(1,0,0),(-1,0,0),(0,0,0),(0,1,0),(0,-1,0),(0,0,-1),(0,0,1)] #Actionlar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndependentAISimuEnv(UavSimuEnv):\n",
    "    \n",
    "    def step(self,uav_index,action_index):\n",
    "        \n",
    "        #Get current and new locations of uav\n",
    "        current_loc = self.getUAV(uav_index).current_location\n",
    "        new_location = [current_loc[i] + self.actions[action_index][i]\n",
    "                                                         for i in range(3)]\n",
    "        \n",
    "        #Calculate Reward\n",
    "        reward = 0\n",
    "        done = False\n",
    "        if(not self.isInside(new_location) or self.isCollides(uav_index,new_location)):\n",
    "            reward = PENALTY\n",
    "            done = True\n",
    "        else:\n",
    "            old_sum = self.calculate_sum_rate()\n",
    "            self.getUAV(uav_index).current_location = new_location\n",
    "            new_sum = self.calculate_sum_rate()\n",
    "            reward = new_sum - old_sum\n",
    "        \n",
    "        #If uav is the last one then increase step\n",
    "        if(uav_index==self.uav_count-1):\n",
    "            self.initial_time+=1\n",
    "            \n",
    "        #Get next state (Next uav's next input = next observation of next agent)\n",
    "        next_obs = self.get_state_input(uav_index)\n",
    "        done = done or self.initial_time == self.step_count\n",
    "        return reward,next_obs,done \n",
    "    \n",
    "    def train_agents(self):\n",
    "        for uav_index in range(self.uav_count):\n",
    "            self.getUAV(uav_index).replay() \n",
    "            print(\"\\nMemory: \\n \",self.getUAV(uav_index).memory[-1])\n",
    "            print(\"\\nEpsilon: \",self.getUAV(uav_index).epsilon,\"\\n\")\n",
    "            if(self.initial_time==self.step_count-1):\n",
    "                self.getUAV(uav_index).adaptiveEGreedy() \n",
    "                    \n",
    "    def step_UAVs(self,save_reward=True,isTest=False,isCollectStep=False):  \n",
    "        \n",
    "        #For each uav, get (observation,action,reward,next_state)\n",
    "        for uav_index in range(self.uav_count):\n",
    "            \n",
    "            observation = self.get_state_input(uav_index) \n",
    "            \n",
    "            #Get action index of uav\n",
    "            action = self.getUAV(uav_index).act(observation, cannot_random=isTest)\n",
    "            \n",
    "            #step env\n",
    "            reward,next_obs,done = self.step(uav_index,action)\n",
    "            \n",
    "            #If training, agent stores\n",
    "            if(not isTest):\n",
    "                self.getUAV(uav_index).store(observation,action,reward,next_obs,done) \n",
    "        \n",
    "        #After a step finishes, agent replays\n",
    "        if(not isTest and not isCollectStep):\n",
    "            self.train_agents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.losses import Huber\n",
    "import tensorflow as tf\n",
    "\n",
    "class DDQNAgent(UAV):\n",
    "    \n",
    "    def update_target_model(self):\n",
    "        # copy weights from model to target_model\n",
    "        weights = self.model.get_weights()\n",
    "        target_weights = self.target_model.get_weights()\n",
    "        for i in range(len(target_weights)):\n",
    "            target_weights[i] = weights[i]\n",
    "        self.target_model.set_weights(target_weights)\n",
    "        \n",
    "    def __init__(self,*args,**kwargs): \n",
    "        super().__init__(*args,**kwargs)\n",
    "        self.target_model = tf.keras.models.clone_model(self.model)\n",
    "        \n",
    "\n",
    "class DDQNAgentDiff(DDQNAgent): \n",
    "    \n",
    "    def replay_with_indexes(self,indexes,next_agent_target):\n",
    "        minibatch = [self.memory[index] for index in indexes]\n",
    "        self.replay_on_batch(minibatch,next_agent_target)\n",
    "    \n",
    "    def replay_on_batch(self,minibatch,next_agent_target):\n",
    "        if(not self.is_memory_enough() ):\n",
    "            return\n",
    "        for state,action, reward,next_state, done in minibatch:  \n",
    "            \n",
    "            if done:\n",
    "                target = reward\n",
    "            else:  \n",
    "                target = reward + self.gamma*np.amax(next_agent_target.predict(next_state))   \n",
    "            train_target = self.model.predict(state)\n",
    "            train_target[0][action] = target\n",
    " \n",
    "            self.model.fit(state,train_target,verbose=1,use_multiprocessing=True)\n",
    "        self.update_target_model()\n",
    "    \n",
    "    def replay(self,next_agent_target):\n",
    "        \n",
    "        if(not self.is_memory_enough() ):\n",
    "            return\n",
    "        minibatch = rand.sample(self.memory,self.batch_size)\n",
    "        self.replay_on_batch(minibatch,next_agent_target)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.losses import Huber\n",
    "from keras.optimizers import RMSprop\n",
    "    \n",
    "class UavSimuEnvWithComm(UavSimuEnv):\n",
    "               \n",
    "    def is_collect_step(self):\n",
    "        return self.getUAV(0).is_memory_enough() \n",
    "               \n",
    "    def get_current_epsilon(self):\n",
    "        return self.getUAV(0).epsilon\n",
    "    \n",
    "    def train_agents(self):\n",
    "        for uav_index in range(self.uav_count):\n",
    "            next_uav_index = (uav_index+1)%(self.uav_count)\n",
    "            next_uav_policy = self.getUAV(next_uav_index).model\n",
    "            self.getUAV(uav_index).replay(next_uav_policy) \n",
    "\n",
    "            print(\"\\nMemory: \\n \",self.getUAV(uav_index).memory[-1])\n",
    "            print(\"\\nEpsilon: \",self.getUAV(uav_index).epsilon,\"\\n\")\n",
    "\n",
    "            if(self.initial_time==self.step_count-1):\n",
    "                self.getUAV(uav_index).adaptiveEGreedy() \n",
    "    \n",
    "    def isCollides(self,uav_index,new_location): \n",
    "        for index,uav in enumerate(self.map[\"uav_set\"]): \n",
    "            if(index>uav_index):\n",
    "                return False\n",
    "            elif(index!=uav_index and np.array_equal(uav.current_location, new_location)):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def get_state_input(self,uav_index):\n",
    "        all_uav_coordinates = []\n",
    "\n",
    "        all_uav_coordinates.extend(self.getUAV(uav_index).current_location[:self.coord_count])\n",
    "        for index,uav in enumerate(self.map[\"uav_set\"]):\n",
    "            if(uav_index==index):\n",
    "                continue \n",
    "            all_uav_coordinates.extend(uav.current_location[:self.coord_count]) \n",
    "            \n",
    "        time_range = [0 for _ in range(math.ceil(self.step_count/UAV_MOVEMENT_SCALE))]\n",
    "        time_range[self.initial_time // UAV_MOVEMENT_SCALE] = 1\n",
    "         \n",
    "        return self.reshape(all_uav_coordinates +time_range)\n",
    "\n",
    "    def step(self,uav_index,action_index):\n",
    "        \n",
    "        #Get current and new locations of uav\n",
    "        current_loc = self.getUAV(uav_index).current_location\n",
    "        new_location = [current_loc[i] + self.actions[action_index][i]\n",
    "                                                         for i in range(3)]\n",
    "        \n",
    "        #Calculate Reward\n",
    "        reward = 0\n",
    "        done = False\n",
    "        if(not self.isInside(new_location) or self.isCollides(uav_index,new_location)):\n",
    "            reward = PENALTY\n",
    "            done = True\n",
    "        else:\n",
    "            old_sum = self.calculate_sum_rate()\n",
    "            self.getUAV(uav_index).current_location = new_location\n",
    "            new_sum = self.calculate_sum_rate()\n",
    "            reward = new_sum - old_sum\n",
    "        \n",
    "        #Get next uav's index\n",
    "        next_uav_index = (uav_index+1)%(self.uav_count)\n",
    "        \n",
    "        #If uav is the last one then increase step\n",
    "        if(uav_index==self.uav_count-1):\n",
    "            self.initial_time+=1\n",
    "            \n",
    "        #Get next state (Next uav's next input = next observation of next agent)\n",
    "        next_obs = self.get_state_input(next_uav_index)\n",
    "        done = done or self.initial_time == self.step_count\n",
    "        return reward,next_obs,done \n",
    "\n",
    "    def step_UAVs(self,save_reward=True,isTest=False,isCollectStep=False):  \n",
    "        \n",
    "        \n",
    "        next_obs = self.get_state_input(0)\n",
    "        #For each uav, get (observation,action,reward,next_state)\n",
    "        for uav_index in range(self.uav_count):\n",
    "            \n",
    "            observation = next_obs.copy()\n",
    "            \n",
    "            #Get action index of uav\n",
    "            action = self.getUAV(uav_index).act(observation, cannot_random=isTest)\n",
    "            \n",
    "            #step env\n",
    "            reward,next_obs,done = self.step(uav_index,action)\n",
    "            \n",
    "            #If training, agent stores\n",
    "            if(not isTest):\n",
    "                self.getUAV(uav_index).store(observation,action,reward,next_obs,done) \n",
    "        \n",
    "        #After a step finishes, agent replays\n",
    "        if(not isTest and not isCollectStep):\n",
    "            self.train_agents()\n",
    "            \n",
    "    def __init__(self,uav_paths,ue_set,env_dim = (100,100),batch_size=200,max_memory_per_agent=10000,uav_class=UAV):\n",
    "        super().__init__(uav_paths,ue_set,env_dim = (100,100),batch_size=batch_size,max_memory_per_agent=max_memory_per_agent,uav_class=uav_class)\n",
    "        self.map[\"uav_set\"] = self.init_uav(uav_paths,batch_size,max_memory_per_agent,uav_class)\n",
    "        \n",
    "\n",
    "class UavSimuEnvWithCommSameReplayIndexesReversedReplay(UavSimuEnvWithComm): \n",
    "        \n",
    "    def train_agents(self):\n",
    "        indexes = rand.sample(range(len(self.getUAV(0).memory)), self.getUAV(0).batch_size)\n",
    "        for uav_index in reversed(range(self.uav_count)):\n",
    "            \n",
    "            next_uav_index = (uav_index+1)%(self.uav_count)\n",
    "            next_uav_policy = self.getUAV(next_uav_index).model\n",
    "            self.getUAV(uav_index).replay_with_indexes(indexes,next_uav_policy) \n",
    "            \n",
    "            print(\"\\nMemory: \\n \",self.getUAV(uav_index).memory[-1])\n",
    "            print(\"\\nEpsilon: \",self.getUAV(uav_index).epsilon,\"\\n\")\n",
    "            \n",
    "            if(self.initial_time==self.step_count-1):\n",
    "                self.getUAV(uav_index).adaptiveEGreedy() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import AveragePooling2D,Flatten, Conv2D, Input, concatenate\n",
    "from random import randint\n",
    "from keras import Model\n",
    "class semiConvAgent(DDQNAgent):\n",
    "    \n",
    "    def build_model(self):\n",
    "        inputA = Input(shape=(self.input_size[0],))\n",
    "        inputB = Input(shape=list(self.input_size[1])+[1])\n",
    "        \n",
    "        uav_coord_input = Dense(64, activation=\"relu\")(inputA)\n",
    "        uav_coord_input = Model(inputs=inputA, outputs=uav_coord_input)\n",
    "        \n",
    "        ue_coord_input = AveragePooling2D(pool_size=(4, 4),padding=\"same\")(inputB)\n",
    "        ue_coord_input = Flatten()(ue_coord_input)\n",
    "        ue_coord_input = Model(inputs=inputB, outputs=ue_coord_input)\n",
    "\n",
    "        combined = concatenate([uav_coord_input.output, ue_coord_input.output])\n",
    "        \n",
    "        z = Dense(512, activation=\"relu\")(combined)\n",
    "        z = Dense(512, activation=\"relu\")(z)\n",
    "        z = Dense(256, activation=\"relu\")(z)\n",
    "        z = Dense(self.action_count,activation=\"linear\")(z)\n",
    "        \n",
    "        model = Model(inputs=[uav_coord_input.input, ue_coord_input.input], outputs=z)\n",
    "        model.compile(Adam(learning_rate = self.lr ,clipnorm=1.0),loss=Huber())\n",
    "        print(model.summary())\n",
    "        return model  \n",
    "    \n",
    "class semiConvAgentCooperative(DDQNAgentDiff,semiConvAgent):\n",
    "    \n",
    "    def build_model(self):\n",
    "        return semiConvAgent.build_model(self)\n",
    "    \n",
    "class UserScalableEnv(IndependentAISimuEnv):\n",
    "    def __init__(self,*args,**kwargs):\n",
    "        super().__init__(*args,**kwargs)\n",
    "        \n",
    "    def get_input_size(self,uav_count):\n",
    "        return (uav_count*self.coord_count,self.env_dim)\n",
    "    \n",
    "    #Point process Distribution\n",
    "    def create_random_user_locations(self):\n",
    "        total_intensity = self.env_dim[0]*self.env_dim[1]*(np.exp(1)-1) / 5\n",
    "        max_intensity = np.exp(1)/5\n",
    "\n",
    "        number_points = np.random.poisson(self.env_dim[0] * self.env_dim[1] * max_intensity)\n",
    "        ue_coords = []\n",
    "        ue_count = len(self.map[\"ue_set\"])\n",
    "        \n",
    "        while(len(ue_coords)<ue_count):\n",
    "            x = randint(0, self.env_dim[1]-4)\n",
    "            y = randint(0, self.env_dim[0]-4)\n",
    "            intensity = np.exp(y / self.env_dim[1]) / 5\n",
    "            if intensity >= np.random.uniform(0, max_intensity):\n",
    "                ue_coords.append([x,y,0])\n",
    "        return ue_coords\n",
    "    \n",
    "    \n",
    "    def reset(self):\n",
    "        def reset_set(key):\n",
    "          for index, val in enumerate(self.map[key]):\n",
    "            self.map[key][index].reset()\n",
    "            \n",
    "        ue_set = []\n",
    "        \n",
    "        if(self.test_mode==True):\n",
    "            with open(TEST_ENV_FILE, 'rb') as data: \n",
    "                ue_set = pickle.load(data)\n",
    "        else:\n",
    "            random_locs = self.create_random_user_locations()\n",
    "            for coord in random_locs:\n",
    "                ue_set.append(UE(coord,env_dim,UavSimuEnv.actions,define_path_first=True,step_count=self.step_count//UAV_MOVEMENT_SCALE))\n",
    "        \n",
    "        self.map[\"ue_set\"] = ue_set\n",
    "        reset_set(\"uav_set\")\n",
    "        self.initial_time = 0 \n",
    "\n",
    "    def get_state_input(self,uav_index):\n",
    "        all_uav_coordinates = []\n",
    "\n",
    "        for uav in self.map[\"uav_set\"]:\n",
    "          all_uav_coordinates.extend(uav.current_location[:self.coord_count]) \n",
    "\n",
    "        user_map = np.zeros((env_dim[0],env_dim[1]))\n",
    "        for user in self.map[\"ue_set\"]:\n",
    "            row = user.current_location[0]\n",
    "            col = user.current_location[1]\n",
    "            user_map[row][col] = 1\n",
    "            \n",
    "        return [self.reshape(np.array(all_uav_coordinates)),self.reshape(user_map)]\n",
    "                   \n",
    "    def reshape(self, data):\n",
    "        return np.reshape(data, (1,) + data.shape) \n",
    "    \n",
    "class UserScalableEnvWithTimeInfo(UserScalableEnv):\n",
    "    \n",
    "    def reshape(self, data):\n",
    "        return np.reshape(data, (1,) + data.shape) \n",
    "    \n",
    "    def get_input_size(self,uav_count):\n",
    "        return (uav_count*self.coord_count+1,self.env_dim)\n",
    "    \n",
    "    def get_state_input(self,uav_index):\n",
    "        all_uav_coordinates = []\n",
    "\n",
    "        for uav in self.map[\"uav_set\"]:\n",
    "          all_uav_coordinates.extend(uav.current_location[:self.coord_count]) \n",
    "\n",
    "        user_map = np.zeros((env_dim[0],env_dim[1]))\n",
    "        for user in self.map[\"ue_set\"]:\n",
    "            row = user.current_location[0]\n",
    "            col = user.current_location[1]\n",
    "            user_map[row][col] = 1\n",
    "            \n",
    "        return [self.reshape(np.array(all_uav_coordinates + [self.step_count-self.initial_time])),self.reshape(user_map)]\n",
    "    \n",
    "\n",
    "class UserScalableEnvWithTimeInfowithComm(UavSimuEnvWithCommSameReplayIndexesReversedReplay,UserScalableEnvWithTimeInfo):\n",
    "    \n",
    "    def get_input_size(self,uav_count):\n",
    "        return UserScalableEnvWithTimeInfo.get_input_size(self,uav_count)\n",
    "    \n",
    "    def get_state_input(self,uav_index):\n",
    "        return UserScalableEnvWithTimeInfo.get_state_input(self,uav_index)\n",
    "    \n",
    "    def create_random_user_locations(self):\n",
    "        return UserScalableEnvWithTimeInfo.create_random_user_locations(self)\n",
    "    \n",
    "    def reset(self):\n",
    "        UserScalableEnvWithTimeInfo.reset(self)\n",
    "    \n",
    "    def reshape(self, data):\n",
    "        return UserScalableEnvWithTimeInfo.reshape(self,data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserScalableEnvWithTimeInfowithCommFix(UserScalableEnvWithTimeInfowithComm):\n",
    "\n",
    "    def get_state_input(self,uav_index):\n",
    "        all_uav_coordinates = []\n",
    "\n",
    "        for uav in self.map[\"uav_set\"]:\n",
    "          all_uav_coordinates.extend(uav.current_location[:self.coord_count]) \n",
    "\n",
    "        user_map = np.zeros((env_dim[0],env_dim[1]))\n",
    "        for user in self.map[\"ue_set\"]:\n",
    "            row = user.current_location[0]%self.env_dim[0]\n",
    "            col = user.current_location[1]%self.env_dim[1]\n",
    "            user_map[row][col] = 1\n",
    "            \n",
    "        return [self.reshape(np.array(all_uav_coordinates + [self.initial_time%20])),self.reshape(user_map)]\n",
    "    \n",
    "    def step(self,uav_index,action_index):\n",
    "        \n",
    "        #Get current and new locations of uav\n",
    "        current_loc = self.getUAV(uav_index).current_location\n",
    "        new_location = [current_loc[i] + self.actions[action_index][i]\n",
    "                                                         for i in range(3)]\n",
    "        \n",
    "        #Calculate Reward\n",
    "        reward = 0\n",
    "        done = False\n",
    "        if(not self.isInside(new_location) or self.isCollides(uav_index,new_location)):\n",
    "            reward = PENALTY\n",
    "            done = True\n",
    "        else:\n",
    "            old_sum = self.calculate_sum_rate()\n",
    "            self.getUAV(uav_index).current_location = new_location\n",
    "            new_sum = self.calculate_sum_rate()\n",
    "            reward = new_sum - old_sum\n",
    "        \n",
    "        #Get next uav's index\n",
    "        next_uav_index = (uav_index+1)%(self.uav_count)\n",
    "        \n",
    "        #If uav is the last one then increase step\n",
    "        if(uav_index==self.uav_count-1):\n",
    "            self.initial_time+=1\n",
    "            \n",
    "            if(self.initial_time%UAV_MOVEMENT_SCALE==0):\n",
    "                self.step_UEs()\n",
    "            \n",
    "        #Get next state (Next uav's next input = next observation of next agent)\n",
    "        next_obs = self.get_state_input(next_uav_index)\n",
    "        done = done or self.initial_time == self.step_count\n",
    "        return reward,next_obs,done "
   ]
  },
  {
   "source": [
    "# Log Function"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hPTl9P2_oEGM",
    "outputId": "6baf0300-2a0e-432e-b9a8-d1b333adb94f"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "import os\n",
    "\n",
    "def get_name(*args,**kwargs):\n",
    "    return \"{}_{}_{}LR_{}batch__step{}_iter{}_epsilondecay{}_epsilonmin{}_gamma{}\".format(kwargs[\"agent_class\"],\n",
    "                                                                     kwargs[\"env\"],\n",
    "                                                                     kwargs[\"lr\"],\n",
    "                                                                     kwargs[\"batch_size\"],\n",
    "                                                                     kwargs[\"step_count\"],\n",
    "                                                                     kwargs[\"iteration_count\"],\n",
    "                                                                     kwargs[\"epsilon_decay\"],\n",
    "                                                                     kwargs[\"epsilon_min\"],\n",
    "                                                                     kwargs[\"gamma\"])\n",
    "def writeLog(*args, **kwargs):\n",
    "    \n",
    "    name = get_name(*args,**kwargs)\n",
    "    with open(\"Logs/\"+name+\".json\", 'w') as f:\n",
    "        json.dump(kwargs, f)\n",
    "        f.write(os.linesep)\n",
    "    return name"
   ]
  },
  {
   "source": [
    "# Determine Initial UAV locations"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JSb3aEfKFPxM"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "env_dim = (100,100)\n",
    "STEP_COUNT= 64\n",
    "\n",
    "alt = ALTITUDE\n",
    "mid_height, mid_width = env_dim[1]/2, env_dim[0]/2\n",
    "\n",
    "\n",
    "uav_paths = [ \n",
    "              [mid_width, mid_height,alt],  \\\n",
    "              [mid_width, mid_height,alt-1], \\\n",
    "              [mid_width, mid_height,alt-2], \n",
    "            ]\n",
    "ue_set = []\n",
    "\n",
    "UNIT = 20\n",
    "tmp = UavSimuEnv(uav_paths,ue_set,env_dim)\n",
    "tmp.render() "
   ]
  },
  {
   "source": [
    "# Parameter Tuning Loop"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "rHRmrGIVcX-L",
    "outputId": "6c3c4c02-a939-4de0-8175-fcf1f71eb475",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "EPSILON_DECAY = 0.962\n",
    "\n",
    "learning_rates = [1e-6]\n",
    "agents = [semiConvAgentCooperative]\n",
    "Envs=[UserScalableEnvWithTimeInfowithCommFix]\n",
    "ITERATION_COUNT = 100\n",
    "STEP_COUNT= 64\n",
    "COLLECT_STEP_COUNT = STEP_COUNT\n",
    "batch_sizes = [10]\n",
    "UAV_MOVEMENT_SCALE = 20\n",
    "UNIT =20\n",
    "PENALTY = -100\n",
    "EPSILON_MIN = 0.1\n",
    "GAMMA=0.95  # Gamma daha da azaltırsa daha iyi sonuçlar çıkabilir\n",
    "\n",
    "combinations = itertools.product(learning_rates,Envs,                              \n",
    "                              batch_sizes,agents)  \n",
    "for comb in combinations:\n",
    "    print(comb) \n",
    "    LR = comb[0] \n",
    "    env = comb[1]\n",
    "    batch_size = comb[2]\n",
    "    agent = comb[3]\n",
    "    \n",
    "    name = get_name(lr=LR,env=env.__name__,env_dim=env_dim, \n",
    "            step_count=STEP_COUNT, iteration_count =ITERATION_COUNT, \n",
    "            uav_movement_scale=UAV_MOVEMENT_SCALE, batch_size=batch_size,\n",
    "            agent_class=agent.__name__, collect_step_size=COLLECT_STEP_COUNT, \n",
    "            epsilon_decay=EPSILON_DECAY, epsilon_min=EPSILON_MIN,\n",
    "            gamma=GAMMA)\n",
    "     \n",
    "    environment = env(uav_paths, ue_set, \n",
    "                        env_dim, batch_size=batch_size, uav_class=agent)\n",
    "\n",
    "    per_iter,per_step = simulate(environment,iteration_count=ITERATION_COUNT, \n",
    "                              step_count=STEP_COUNT, batch_size = batch_size, \n",
    "                              collect_step_size=COLLECT_STEP_COUNT, env_name = name)\n",
    "\n",
    "    environment.save(name+\"_endofsim\")\n",
    "\n",
    "    writeLog(lr=LR, env=env.__name__, per_iter=per_iter, per_step=per_step, env_dim=env_dim, \n",
    "          step_count=STEP_COUNT, iteration_count =ITERATION_COUNT, uav_movement_scale=UAV_MOVEMENT_SCALE,\n",
    "          LastMap = environment.get3DMap(), batch_size=batch_size, agent_class=agent.__name__,\n",
    "            collect_step_size=COLLECT_STEP_COUNT, epsilon_decay=EPSILON_DECAY, epsilon_min=EPSILON_MIN, gamma=GAMMA) "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Bitirme.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "interpreter": {
   "hash": "38740d3277777e2cd7c6c2cc9d8addf5118fdf3f82b1b39231fd12aeac8aee8b"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}